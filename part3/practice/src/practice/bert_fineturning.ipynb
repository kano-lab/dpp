{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livedoorニュースコーパスを用いたニューストピック分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### タスクの説明\n",
    "LiveDoorニュースコーパスを用いて、ニュース記事のトピック分類を行います。  \n",
    "本資料ではすでに加工済みのデータを利用しています。  \n",
    "元データのダウンドードは[こちらから](https://www.rondhuit.com/download.html)行えます。  \n",
    "\n",
    "このコーパスは各記事がどのトピックに属するかがラベル付けされています。\n",
    "このコーパスに含まれるトピックは以下の通りです。\n",
    "- トピックニュース\n",
    "- Sports Watch\n",
    "- ITライフハック\n",
    "- 家電チャンネル\n",
    "- MOVIE ENTER\n",
    "- 独女通信\n",
    "- エスマックス\n",
    "- livedoor HOMME\n",
    "- Peachy\n",
    "\n",
    "今回実装するタスクは、ニュース記事の本文から上記の九つのトピックのうちどれに属するかを予測するマルチクラス分類です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUが複数台搭載されている実行環境の場合、以下の環境変数を設定することで、使用するGPUを指定できます\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'Python 3.10.11' でセルを実行するには、 ipykernel パッケージが必要です。\n",
      "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
      "\u001b[1;31mコマンド: '/Users/mizuki/.pyenv/versions/3.10.11/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "BASE_PATH = '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "# NOTE: 仮想環境が適切に構築されていることを事前に確認して下さい\n",
    "!pip install pandas transformers torch fugashi ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed値の設定\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Livedoorニュースコーパスの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_PATH+\"categories.json\",\"r\") as f:\n",
    "    d = f.read()\n",
    "    import json\n",
    "    categories = json.loads(d)\n",
    "num_labels = len(categories)\n",
    "print(categories)\n",
    "print(f\"num_labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(BASE_PATH+\"train.csv\")\n",
    "df_valid = pd.read_csv(BASE_PATH+\"valid.csv\")\n",
    "df_test = pd.read_csv(BASE_PATH+\"test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizerの読み込み\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "\n",
    "# モデルの最大入力トークン数\n",
    "max_length = tokenizer.model_max_length\n",
    "print(f\"model input max_length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回用意されたcsvの形式が読み込まれたdata frameからtokenize後のデータセットを作成する関数を定義\n",
    "import random\n",
    "def create_dataset(df):\n",
    "    dataset = []\n",
    "    for i in range(len(df)):\n",
    "        label = df.iloc[i, 0]\n",
    "        text = df.iloc[i, 4]\n",
    "        if label == \"9\" & random.randint(1, 10) > 1:\n",
    "            continue\n",
    "        dataset.append({\n",
    "            \"label\": label,\n",
    "            \"text\": text,\n",
    "            \"encoding\": tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        })\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだデータフレームから１レコードが辞書型として格納されている配列を作成\n",
    "print(\"creating train dataset...(this may take a while)\")\n",
    "train_dataset = create_dataset(df_train)\n",
    "print(\"creating valid dataset...\")\n",
    "valid_dataset = create_dataset(df_valid)\n",
    "print(\"creating test dataset...\")\n",
    "test_dataset = create_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットを確認\n",
    "print(f\"Number of samples: {len(train_dataset)}\")\n",
    "print(f\"Sample 0: {train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaderの作成\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64  # Hyperparameter\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=num_labels)\n",
    "# モデルの確認\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "lr = 1e-5 # Hyperparameter\n",
    "optimizer_parameters = model.parameters()\n",
    "# optimizerの設定(Adam)\n",
    "optimizer = torch.optim.Adam(optimizer_parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習を行うデバイスを設定\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "losses = []\n",
    "\n",
    "epoch_num = 3 # Hyperparameter\n",
    "\n",
    "step_num = 1\n",
    "epochs = 1\n",
    "\n",
    "last_loss = float('inf')\n",
    "loss_threshold = 0.02 # Hyperparameter\n",
    "\n",
    "valid_step_period = len(train_dataloader) // 5\n",
    "\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "print(\"train start!!!\")\n",
    "for _ in range(epoch_num):\n",
    "    print(f'########### Epoch {epochs} ###########')\n",
    "    # train step\n",
    "    for data in train_dataloader:\n",
    "        # optimizerの初期化\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = data[\"encoding\"][\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = data[\"encoding\"][\"attention_mask\"].squeeze(1).to(device)\n",
    "        token_type_ids = data[\"encoding\"][\"token_type_ids\"].squeeze(1).to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "    \n",
    "        model_output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        logit = model_output.logits\n",
    "        loss = model_output.loss\n",
    "        output_label = torch.argmax(logit, dim=1)\n",
    "    \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        last_loss = loss.item()\n",
    "        step_num += 1\n",
    "    \n",
    "        # validation step\n",
    "        if step_num % valid_step_period == 0:\n",
    "            valid_output_labels = []\n",
    "            valid_correct_labels = []\n",
    "            model.eval()\n",
    "            for data in valid_dataloader:\n",
    "                labels = data[\"label\"].to(device)\n",
    "                \n",
    "                model_output = model(**{k: v.squeeze(1).to(device) for k, v in data[\"encoding\"].items()}, labels=labels)\n",
    "                # モデルの最終層の出力\n",
    "                logit = model_output.logits\n",
    "                # モデルの最終層の出力が最も高いラベルを予測ラベルとする\n",
    "                output_label = torch.argmax(logit, dim=1)\n",
    "\n",
    "                valid_output_labels.append(output_label)\n",
    "                valid_correct_labels.append(labels)\n",
    "\n",
    "            # accuracyの算出：モデルの出力ラベルと正解ラベルが一致している数を全データ数で割った値を算出\n",
    "            valid_accuracy = torch.sum(torch.cat(valid_output_labels) == torch.cat(valid_correct_labels)).item() / len(valid_dataset)\n",
    "            print(f\"Step {step_num} / {len(train_dataloader)}, Last Loss {last_loss}, Valid Accuracy {valid_accuracy}\")\n",
    "            # モデルを学習モードに戻す\n",
    "            model.train()\n",
    "\n",
    "        # early stopping\n",
    "        if last_loss < loss_threshold:\n",
    "            print(\"Loss is less than threshold!\")\n",
    "            break\n",
    "    epochs += 1\n",
    "    step = 0\n",
    "print(\"train finish!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test step\n",
    "test_correct_num = 0\n",
    "test_total_num = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "for data in tqdm(test_dataloader):\n",
    "    labels = data[\"label\"].to(device)\n",
    "\n",
    "    model_output = model(**{k: v.squeeze().to(device) for k,v in data[\"encoding\"].items()}, labels=labels)\n",
    "    logit = model_output.logits\n",
    "    output_label = torch.argmax(logit, dim=1)\n",
    "\n",
    "    test_correct_num += torch.sum(output_label == labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_correct_num / test_total_num\n",
    "print(f\"correct samples | total samples : {test_correct_num} | {test_total_num}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "model_dir = BASE_PATH+\"model/\"\n",
    "model_full_path = model_dir + \"model.pth\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), model_full_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
