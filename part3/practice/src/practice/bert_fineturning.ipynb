{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Livedoorニュースコーパスを用いたニューストピック分類"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### タスクの説明\n",
                "LiveDoorニュースコーパスを用いて、ニュース記事のトピック分類を行います。  \n",
                "本資料ではすでに加工済みのデータを利用しています。  \n",
                "元データのダウンドードは[こちらから](https://www.rondhuit.com/download.html)行えます。  \n",
                "\n",
                "このコーパスは各記事がどのトピックに属するかがラベル付けされています。\n",
                "このコーパスに含まれるトピックは以下の通りです。\n",
                "- トピックニュース\n",
                "- Sports Watch\n",
                "- ITライフハック\n",
                "- 家電チャンネル\n",
                "- MOVIE ENTER\n",
                "- 独女通信\n",
                "- エスマックス\n",
                "- livedoor HOMME\n",
                "- Peachy\n",
                "\n",
                "今回実装するタスクは、ニュース記事の本文から上記の九つのトピックのうちどれに属するかを予測するマルチクラス分類です。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: ご自身のDrive上のファイルパスに変更してください。\n",
                "BASE_PATH = '/content/drive/MyDrive/'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 必要なライブラリのインストール\n",
                "# NOTE: 仮想環境が適切に構築されていることを事前に確認して下さい\n",
                "!pip install pandas transformers torch fugashi ipadic unidic-lite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# seed値の設定\n",
                "from transformers import set_seed\n",
                "seed = 42\n",
                "set_seed(seed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Livedoorニュースコーパスの読み込み"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(BASE_PATH+\"categories.json\",\"r\") as f:\n",
                "    d = f.read()\n",
                "    import json\n",
                "    categories = json.loads(d)\n",
                "num_labels = len(categories)\n",
                "print(categories)\n",
                "print(f\"num_labels: {num_labels}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### データセットの準備"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "df_train = pd.read_csv(BASE_PATH+\"train.csv\")\n",
                "df_valid = pd.read_csv(BASE_PATH+\"valid.csv\")\n",
                "df_test = pd.read_csv(BASE_PATH+\"test.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(df_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### BERTにおける tokenizeについて\n",
                "tokenizerに東北大学が作成した事前学習済み日本語BERTモデルであるcl-tohoku/bert-base-japanese-whole-word-maskingを使用する。また、学習を行う際も同様のモデルを使用する。\n",
                "\n",
                "*   **whole-word-maskingとは何か**<br>\n",
                "文中のランダムなトークン（単語の一部または全体）が選ばれ、そのトークンが「マスク」される（隠される）と、モデルはそのマスクされた部分を予測しようとする。<br>\n",
                "しかし、一部の単語は分割されることなく複数のトークンに分割されることがある。たとえば、 \"プロ野球\" は \"プロ\" と \"野球\" のように分割される場合がある。通常のマスキングではこれらが別々にマスクされる可能性があるが、Whole-word masking では「単語全体」が一度にマスクされる。つまり、\"プロ野球\" 全体がマスク対象になり、モデルがより自然な単語の境界で学習する助けとなる。<br>\n",
                "<br>\n",
                "例：<br>\n",
                "学習対象文字列<br>\n",
                "私の同級生はプロ野球選手です。<br>\n",
                "<br>\n",
                "従来方式の学習（マスキングの仕方がランダム）<br>\n",
                "[MASK]の同級生はプロ野球[MASK]です。<br>\n",
                "<br>\n",
                "whole-word-maskingでの学習（一連の単語は連続してマスキング）<br>\n",
                "[MASK]の同級生は[MASK][MASK][MASK]です。<br>\n",
                "<br>\n",
                "<br>\n",
                "[参考]<br>\n",
                "https://a16mixx.com/bert%E3%81%AE%E6%97%A5%E6%9C%AC%E8%AA%9E%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%A8%AE%E9%A1%9E/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenizerの読み込み\n",
                "from transformers import BertJapaneseTokenizer\n",
                "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
                "\n",
                "# モデルの最大入力トークン数\n",
                "max_length = tokenizer.model_max_length\n",
                "print(f\"model input max_length: {max_length}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### tokenizer()の返り値について\n",
                "* **input_ids**<br>\n",
                "各トークンのid<br>\n",
                "* **token_type_ids**<br>\n",
                "2つの文を入力する際に重要となる項目で、1つ目の文に含まれるトークンに対しては「0」が、2つ目の文に含まれるトークンに対しては「1」が割り振られる。<br>\n",
                "今回は1つの文のみの入力となるため、すべて「0」となる。<br>\n",
                "* **attention_mask**<br>\n",
                "トークンの場合1、トークンでない場合0を返す。<br>\n",
                "<br>\n",
                "<br>\n",
                "[参考]<br>\n",
                "https://zenn.dev/robes/articles/b6708032855a9c"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 今回用意されたcsvの形式が読み込まれたdata frameからtokenize後のデータセットを作成する関数を定義\n",
                "import random\n",
                "def create_dataset(df):\n",
                "    dataset = []\n",
                "    for i in range(len(df)):\n",
                "        label = df.iloc[i, 0] # DataFrameオブジェクトに対してインデックス指定をするために、df.iloc[]を使用\n",
                "        text = df.iloc[i, 4]\n",
                "        dataset.append({\n",
                "            \"label\": label,\n",
                "            \"text\": text,\n",
                "            # truncation=True: max_lengthトークンを超える場合は切り捨て、return_tensors=\"pt\": PyTorchのテンソル形式で返す\n",
                "            \"encoding\": tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
                "        })\n",
                "    return dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 読み込んだデータフレームから１レコードが辞書型として格納されている配列を作成\n",
                "print(\"creating train dataset...(this may take a while)\")\n",
                "train_dataset = create_dataset(df_train)\n",
                "print(\"creating valid dataset...\")\n",
                "valid_dataset = create_dataset(df_valid)\n",
                "print(\"creating test dataset...\")\n",
                "test_dataset = create_dataset(df_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# データセットを確認\n",
                "print(f\"Number of samples: {len(train_dataset)}\")\n",
                "print(f\"Sample 0: {train_dataset[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataloaderの作成\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "batch_size = 64  # Hyperparameter\n",
                "\n",
                "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
                "# drop_last=True:データセットのサイズがバッチサイズで割り切れない場合、最後に余りのデータを使用せずに捨てる\n",
                "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
                "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
                "\n",
                "print(f\"Number of training samples: {len(train_dataset)}\")\n",
                "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
                "print(f\"Number of test samples: {len(test_dataset)}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### モデルの定義"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import BertForSequenceClassification\n",
                "model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=num_labels)\n",
                "# モデルの確認\n",
                "print(model.classifier)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "lr = 1e-5 # Hyperparameter\n",
                "optimizer_parameters = model.parameters()\n",
                "# optimizerの設定(Adam)\n",
                "optimizer = torch.optim.Adam(optimizer_parameters, lr=lr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 学習を行うデバイスを設定\n",
                "if torch.cuda.is_available():\n",
                "    device = 'cuda'\n",
                "else:\n",
                "    device = 'cpu'\n",
                "# ATTENTION: deviceがcudaになっていることを確認してください！\n",
                "print(f\"device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "losses = []\n",
                "\n",
                "epoch_num = 3 # Hyperparameter\n",
                "\n",
                "step_num = 1\n",
                "epochs = 1\n",
                "\n",
                "last_loss = float('inf')\n",
                "loss_threshold = 0.02 # Hyperparameter\n",
                "\n",
                "valid_step_period = len(train_dataloader) // 5\n",
                "\n",
                "model.train()\n",
                "model.to(device)\n",
                "\n",
                "print(\"train start!!!\")\n",
                "for _ in range(epoch_num):\n",
                "    print(f'########### Epoch {epochs} ###########')\n",
                "    # train step\n",
                "    for data in train_dataloader:\n",
                "        # optimizerの初期化\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        input_ids = data[\"encoding\"][\"input_ids\"].squeeze(1).to(device)\n",
                "        attention_mask = data[\"encoding\"][\"attention_mask\"].squeeze(1).to(device)\n",
                "        token_type_ids = data[\"encoding\"][\"token_type_ids\"].squeeze(1).to(device)\n",
                "        labels = data[\"label\"].to(device)\n",
                "    \n",
                "        model_output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
                "        logit = model_output.logits\n",
                "        loss = model_output.loss\n",
                "        output_label = torch.argmax(logit, dim=1)\n",
                "    \n",
                "        losses.append(loss.item())\n",
                "    \n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        last_loss = loss.item()\n",
                "        step_num += 1\n",
                "    \n",
                "        # validation step\n",
                "        if step_num % valid_step_period == 0:\n",
                "            valid_output_labels = []\n",
                "            valid_correct_labels = []\n",
                "            model.eval()\n",
                "            for data in valid_dataloader:\n",
                "                labels = data[\"label\"].to(device)\n",
                "                \n",
                "                model_output = model(**{k: v.squeeze(1).to(device) for k, v in data[\"encoding\"].items()}, labels=labels)\n",
                "                # モデルの最終層の出力\n",
                "                logit = model_output.logits\n",
                "                # モデルの最終層の出力が最も高いラベルを予測ラベルとする\n",
                "                output_label = torch.argmax(logit, dim=1)\n",
                "\n",
                "                valid_output_labels.append(output_label)\n",
                "                valid_correct_labels.append(labels)\n",
                "\n",
                "            # accuracyの算出：モデルの出力ラベルと正解ラベルが一致している数を全データ数で割った値を算出\n",
                "            valid_accuracy = torch.sum(torch.cat(valid_output_labels) == torch.cat(valid_correct_labels)).item() / len(valid_dataset)\n",
                "            print(f\"Step {step_num} / {len(train_dataloader)}, Last Loss {last_loss}, Valid Accuracy {valid_accuracy}\")\n",
                "            # モデルを学習モードに戻す\n",
                "            model.train()\n",
                "\n",
                "        # early stopping\n",
                "        if last_loss < loss_threshold:\n",
                "            print(\"Loss is less than threshold!\")\n",
                "            break\n",
                "    epochs += 1\n",
                "    step_num = 0\n",
                "print(\"train finish!!!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# test step\n",
                "test_correct_num = 0\n",
                "test_total_num = len(test_dataset)\n",
                "\n",
                "model.eval()\n",
                "for data in tqdm(test_dataloader):\n",
                "    labels = data[\"label\"].to(device)\n",
                "\n",
                "    model_output = model(**{k: v.squeeze(1).to(device) for k,v in data[\"encoding\"].items()}, labels=labels)\n",
                "    logit = model_output.logits\n",
                "    output_label = torch.argmax(logit, dim=1)\n",
                "\n",
                "    test_correct_num += torch.sum(output_label == labels).item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "accuracy = test_correct_num / test_total_num\n",
                "print(f\"correct samples | total samples : {test_correct_num} | {test_total_num}\")\n",
                "print(f\"Accuracy: {accuracy}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# モデルの保存\n",
                "model_path = BASE_PATH + \"model.pth\"\n",
                "torch.save(model.state_dict(), model_path)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
