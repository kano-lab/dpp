{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ処理プログラミング 第3回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Livedoorニュースコーパスで学習されたモデルをもとに、ニュース記事のカテゴリを予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch fugashi ipadic pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストデータの読み込み\n",
    "- 読み込むファイル\n",
    "    - `test.csv`  \n",
    "    モデルのテストに利用するためのデータ。カラムと内容は以下の通りです。\n",
    "    - label: ニュース記事のカテゴリを表すラベルを数字で表したもの。同じカテゴリーの記事は同じ数字になっています。\n",
    "    - url: ニュース記事のURL\n",
    "    - date: ニュース記事の日付\n",
    "    - category: ニュース記事のカテゴリ\n",
    "    - body: ニュース記事本文\n",
    "- 読み込み後の形式  \n",
    "    以下のような配列形式でデータを読み込んでください。\n",
    "    なお、最終的にデータが格納される配列の変数名は`test_dataset`としてください。\n",
    "    ```\n",
    "    [\n",
    "        {\n",
    "            \"label\": ラベル, \n",
    "            \"text\": ニュース記事本文\n",
    "        },\n",
    "        {\n",
    "            \"label\": ラベル, \n",
    "            \"text\": ニュース記事本文\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 以下にテストデータを上記の形式で読み込むコードを書いてください\n",
    "import pandas as pd\n",
    "df = pd.read_csv(BASE_PATH+'test.csv')\n",
    "test_dataset = []\n",
    "for i in range(len(df)):\n",
    "    label = df.iloc[i, 0]\n",
    "    text = df.iloc[i, 4]\n",
    "    test_dataset.append({\n",
    "        'label': label,\n",
    "        'text': text\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category情報が格納されたファイルの読み込み\n",
    "- 読み込むファイル\n",
    "    - `categories.json`  \n",
    "    ラベルのカテゴリ情報が格納されたファイル。\n",
    "    以下のような形式でデータが格納されています。\n",
    "    ```\n",
    "    [\n",
    "        {\n",
    "            \"label\": ラベル名(数字),\n",
    "            \"category\": カテゴリ名,\n",
    "        }\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    ```\n",
    "- 読み込み後の形式  \n",
    "    そのままのPython dict型の要素が格納された配列で読み込んでください。\n",
    "    配列が格納される変数名は`categories`としてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 以下にcategory情報を上記の形式で読み込むコードを書いてください\n",
    "import json\n",
    "with open(BASE_PATH+'categories.json') as f:\n",
    "    categories = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed値の設定\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: トークナイザを読み込むコードを書いて下さい。\n",
    "# トークナイザは「'cl-tohoku/bert-base-japanese-whole-word-masking'」用のモデルを利用して下さい。この名称+tokenizer等で検索するか、学習スクリプトを参考にしてみて下さい。\n",
    "# なお、読み込んだtokenizerはtokenzierという変数に格納して下さい。\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_length = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test_datasetの各辞書型のデータをfor文で回し、辞書型のデータ内の'text'キーの値をトークナイザに入力し、その結果を元の辞書型のデータに'encoding'キーで追加してください。\n",
    "# tokenizerに渡すパラメーターは以下のものを利用して下さい。\n",
    "# padding='max_length', truncation=True, max_length=model_max_length, return_tensors='pt'\n",
    "for data in test_dataset:\n",
    "    data['encoding'] = tokenizer(data['text'], padding='max_length', truncation=True, max_length=model_max_length, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# データセットの確認\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境の設定\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "model_path = BASE_PATH+'model/model.pth'\n",
    "model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=len(categories))\n",
    "# 学習済みモデルの重みを読み込む\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval() # モデルを評価モードにする\n",
    "model.to(device) # モデルをGPUに載せる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータのDataLoaderを作成\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用のスクリプト\n",
    "from tqdm import tqdm\n",
    "texts = []\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "for data in tqdm(test_dataloader):\n",
    "    label = data['label']\n",
    "    encoding = data['encoding']\n",
    "    output = model(**{k: v.squeeze(0).to(device) for k, v in encoding.items()})\n",
    "    \n",
    "    pred_label = torch.argmax(output.logits, dim=1)\n",
    "    pred_labels.append(pred_label.item())\n",
    "    true_labels.append(label)\n",
    "    texts.append(data['text'])\n",
    "assert len(pred_labels) == len(true_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (torch.tensor(pred_labels) == torch.tensor(true_labels)).float().sum().item() / len(true_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実際に予測されたラベルと正解ラベルを表示\n",
    "for i in range(5):\n",
    "    print(f\"Text: {texts[i]}\")\n",
    "    print(f\"model output: {categories[pred_labels[i]]}\")\n",
    "    print(f\"True label: {categories[true_labels[i]]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2(デコーダーモデル)を触ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\", use_fast=False)\n",
    "gpt_tokenizer.do_lower_case = True  \n",
    "\n",
    "decoder_model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "decoder_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各種パラメーターの説明\n",
    "*   **max_length**<br>\n",
    "生成されるテキストの最大長<br>\n",
    "*   **top_k**<br>\n",
    "出力候補の上位何％からランダムにピックアップするかを決めるパラメーター<br>\n",
    "*   **temperature**<br>\n",
    "高い確率の単語の可能性を増加させ、低確率の単語の可能性を減少させる。<br>\n",
    "*   **do_sample**<br>\n",
    "Top-Kサンプリングなどのランダムサンプリングを使用するかどうか。<br>\n",
    "*   **num_beams**<br>\n",
    "1よりも大きな値を指定することで、貪欲検索からビームサーチに切り替えられる。<br>\n",
    "ビームサーチは、貪欲検索よりも多くの仮説を考慮する検索手法。この方法では、各ステップで複数の仮説を評価し、最終的にな確率が高くなるものを選びぶ。<br>\n",
    "これにより、初期の確率が低いトークンで始まる高確率のシーケンスが貪欲検索によって無視されることがなくなる。<br>\n",
    "*   **early_stopping**<br>\n",
    "ビーム検索でEOSトークンに到達したときに生成が終了するように設定<br>\n",
    "*   **no_repeat_ngram_size**<br>\n",
    "指定されたngramサイズの繰り替えしへの制約を与える<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "[参考]<br>\n",
    "https://huggingface.co/docs/transformers/ja/generation_strategies<br>\n",
    "https://zenn.dev/tyaahan/articles/a8d99900000002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章生成時のパラメータ\n",
    "# TODO: パラメーターの値を変えたり追加したりして、モデルの出力がどのように変化するかをみてみましょう。各種パラメーターがどのような働きなのかを理解しながら変えてみるのが良いかもしれません。\n",
    "params = {\n",
    "    \"max_length\":100,\n",
    "    \"top_k\" : 50,\n",
    "    \"temperature\": 0.7,\n",
    "    \"do_sample\": True,\n",
    "    # \"num_beams\":5,\n",
    "    # \"early_stopping\":True,\n",
    "    # \"no_repeat_ngram_size\":2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: start_textを色々変えてみると...?????\n",
    "start_text = \"昔々あるところに、\"\n",
    "with torch.no_grad():\n",
    "    input_ids = gpt_tokenizer.encode(start_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    output_ids = decoder_model.generate(input_ids.to(device), pad_token_id=gpt_tokenizer.pad_token_id, **params)\n",
    "\n",
    "print(gpt_tokenizer.decode(output_ids.tolist()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
